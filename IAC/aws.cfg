[default]
aws_access_key_id = "AKIAYNK4YEZGQDC3UQ5K"
aws_secret_access_key = "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"

from pyspark.sql import SparkSession
import os

def spark_session_create():
    spark = SparkSession.builder.\
    config("spark.jars.repositories", "https://repos.spark-packages.org/").\
    config("spark.jars.packages", "saurfang:spark-sas7bdat:2.0.0-s_2.11").\
    enableHiveSupport().getOrCreate()
    return spark
def process_immigration_data(spark,input_data,output_data):
    #immigration_df = spark.read.csv(input_data,'immigration_data_sample.csv',header=True)
    file_path = f'{input_data}/immigration_data_sample.csv'
    print(f'Reading from {file_path}')
    immigration_df = spark.read.option('header', 'true').csv(file_path)
    immigration_df.printSchema()
    immigration_df.show()
    print("Imigration daata read completed successfuly ----------------")
    #output_path = os.path.join(output_data, "immigration_data_in_parquet")
    immigration_df.dropDuplicates().count()
    #immigration_df.write.parquet('immigration_data_in_parquet')
    immigration_df.write.mode("overwrite").parquet(output_data)
def main():
    input_data="file:///workspace/home/capstone_input"
    output_data="/workspace/home/capstone_output/"
    spark=spark_session_create()
    print("------------------- Spark Session Create")
    process_immigration_data(spark,input_data,output_data)

if __name__ == "__main__":
    main()
